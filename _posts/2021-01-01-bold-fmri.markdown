@@ -1,157 +0,0 @@
---
layout: post
title:  "BOLD fMRI"
date:   2020-01-02 11:11:03 -0400
categories: medical imaging
---
## Motivation
Blood oxygen level dependent functional magnetic resonance imaging (BOLD fMRI) is the most common method for measuring human brain activity non-invasively in-vivo. BOLD fMRI images are 4-dimensional, consisting of a time series of 3d volume, acquired in quick succession (every 1 or 2 seconds) typically over a period of 8 --- 15 minutes.

Here, we will work with "[*multisubject, multimodal face processing*](https://openneuro.org/datasets/ds000117/versions/1.0.4)" dataset (subject-01) available at *openneuro.org*. This dataset involves presentation of images of faces to the subject while acquiring BOLD fMRI images of the subject's brain activity. Here, we will preprocess these scans and then, localize the brain area that processes faces.

## Preparation
We need both *afni* and *FSL* software packages. Install VirtualBox and then download [*Neurodebian*](https://neuro.debian.net/) and open it in VirtualBox by selecting `File->Import Appliance`. *Neurodebian* is a Linux distribution of the Debian flavor.

Once installed, we will be able to run the preprocessing pipeline on the data from *openneuro*.

## Basic pre-preprocessing
Basic preprocessing includes motion correction, bandpass filtering, spatial smoothing. Run pre-processing pipeline [*pipeline.sh*](https://github.com/zyz9066/Image-Analysis/blob/master/BOLD%20fMRI/pipeline.sh) from the command terminal. The pipeline will take roughly 5 --- 10 minutes to run, depending on hardware setup. The processing pipeline will produce a final output called *clean_bold.nii.gz*, we will use this image for further work.

![](https://zyz9066.github.io/images/516/3/ScreenShot.png)

Above is a screenshot of the directory after running *pipeline.sh*.

## Localize task activation
Briefly, task-based analysis procedure is as follows:
1. Clean the BOLD images (using *pipeline.sh*);
2. Load the *clean_bold.nii.gz* image output by *pipeline.sh* and *events.tsv* which corresponding to each BOLD image;
3. Using the timing form *events.tsv*, create an "ideal time series" that represents how the brain should react to the stimulus (face=1, no face=0);

```python
import nibabel as nib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

clean_bold = nib.load('/clean_bold.nii.gz')
events = pd.read_csv('/events.tsv', delimiter='\t')
tr = clean_bold.header.get_zooms()[3]

ts = np.zeros(int(tr * clean_bold.shape[3]))

ts[np.round(events[~events['stim_type'].isna()]['onset'].values).astype('uint16')] = 1

plt.plot(ts); plt.xlabel('time(seconds)');
```

![](https://zyz9066.github.io/images/516/3/ts.png)

4. Convolve the "ideal time series" the [hemodynamic response function (HRF)](https://github.com/zyz9066/Image-Analysis/blob/master/BOLD%20fMRI/hrf.csv), we can use *pandas* load *hrf.csv* `hrf = pd.read_csv('hrf.csv', header=None)`;
```python
hrf = pd.read_csv('hrf.csv', header=None).values.ravel()
plt.plot(hrf); plt.xlabel('time(seconds)');
```

![](https://zyz9066.github.io/images/516/3/hrf.png)

```python
import scipy.signal as signal

conved = signal.convolve(ts, hrf, mode='full')[:ts.shape[0]]

plt.plot(ts); plt.plot(conved*3); plt.xlabel('time(seconds)');
```

![](https://zyz9066.github.io/images/516/3/cts.png)

5. Correlate the convolved ideal with the BOLD signal in each voxel;

```python
conved = conved[::int(tr)]
img = clean_bold.get_fdata()

meansub_img = img - np.expand_dims(img.mean(-1), 3)
meansub_conved = conved - conved.mean()

corrs = (meansub_img * meansub_conved).sum(-1) / \
    np.sqrt((meansub_img * meansub_img).sum(-1)) / np.sqrt(np.dot(meansub_conved, meansub_conved))

corrs[np.isnan(corrs)] = 0

corrs_nifti = nib.Nifti1Image(corrs, clean_bold.affine)
nib.save(corrs_nifti, 'corrs.nii.gz')
```

```python
from sklearn.metrics import mutual_info_score

def MI(a):
    c_xy = np.histogram2d(a, conved)[0]
    return mutual_info_score(None, None, contingency=c_xy)

mi = np.apply_along_axis(MI, 3, img)

mi_nifti = nib.Nifti1Image(mi, clean_bold.affine)
nib.save(mi_nifti, path+str(i)+'/mi.nii.gz')
```

6. Visualize the correlation map to see where in the brain the activation is strongest.

```python
plt.imshow(np.rot90(corrs.max(2))); plt.colorbar();
plt.imshow(np.rot90(corrs.max(2)), vmin=-0.25, vmax=0.25); plt.colorbar();
```

![](https://zyz9066.github.io/images/516/3/corr.png)

![](https://zyz9066.github.io/images/516/3/corrv.png)

```python
plt.imshow(np.rot90(mi.max(2))); plt.colorbar();
plt.imshow(np.rot90(mi.max(2)), vmin=-0.25, vmax=0.25); plt.colorbar();
```

![](https://zyz9066.github.io/images/516/3/mi.png)

![](https://zyz9066.github.io/images/516/3/miv.png)

Use the above steps, we find the brain area which correlates to viewing of faces. We displayed a figure with maximum z-slice among 46 slices and a final activation map using `imshow` with `vmin=-0.25` and `vmax=0.25`. There are some clusters near the back of the brain with high correlation values.

### No pre-processing
Repeat the above, but leave out step 1 (use *bold.nii.gz*, without pre-processing, instead of the *clean_bold.nii.gz* output by *pipeline.sh*):

```python
raw = nib.load('bold.nii.gz').get_fdata()

meansub_raw = raw - np.expand_dims(raw.mean(-1), 3)

corrs_raw = (meansub_raw * meansub_conved).sum(-1) / \
    np.sqrt((meansub_raw * meansub_raw).sum(-1)) / np.sqrt(np.dot(meansub_conved, meansub_conved))

corrs_raw[np.isnan(corrs_raw)] = 0
plt.imshow(np.rot90(corrs_raw.max(2))); plt.colorbar();
plt.imshow(np.rot90(corrs_raw.max(2)), vmin=-0.25, vmax=0.25); plt.colorbar();
```

![](https://zyz9066.github.io/images/516/3/corrn.png)

![](https://zyz9066.github.io/images/516/3/corrnv.png)

```python
mi_raw = np.apply_along_axis(MI, 3, raw)

plt.imshow(np.rot90(mi_raw.max(2))); plt.colorbar();
plt.imshow(np.rot90(mi_raw.max(2)), vmin=-0.25, vmax=0.25); plt.colorbar();
```

![](https://zyz9066.github.io/images/516/3/min.png)

![](https://zyz9066.github.io/images/516/3/minv.png)

The correlations are stronger when pre-processing is included.

## Multi-subject analysis
The dataset on *openneuro* contains scans from 16 subjects. In previous part, we only processed data from *sub-01*. Each subject actually contains multiple BOLD fMRI runs, here we just use T1 and first fMRI from all 16 subjects. However, we may use all the BOLD scans from each subject and average the resulting correlation maps to produce a more clear correlation.

### Batch processing
Once we have all subjects, create a separate folder for each subject and place each subjects' data in their own separate folder. Then, run the pre-processing pipeline on each subject separately (this may take up to 1 hour). We can simply place the script in each subject's folder and run it as-is at the command line, or create a `for` loop surrounding the pre-processing code in the *pipeline.sh* file to loop over all subjects automatically.

Once each subject has been pre-processed, run the correlation analysis as in previous part on each subject and save the output as a *.nii* file into the same subject's directory. Be sure to keep the same header transform as the input image when saving using *nibabel*, otherwise the correlation map will not be in the same space (see below):
